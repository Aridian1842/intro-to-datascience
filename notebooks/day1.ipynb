{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1\n",
    "\n",
    "## Module 2\n",
    "\n",
    "In this module you will learn how to load the online retail dataset in Python, visualise and summarise it to produce insights that will guide your machine learning endevours in the next modules. We will learn how to plot data and calculate summary statistics to build a dataset from which we ultimately will try to predict future customer behaviour.\n",
    "\n",
    "### Learning Activity - Loading Libraries\n",
    "\n",
    "First we need to load the required Python libraries. Libraries are like extensions to the base `python` that add functionality or help to make tasks more convenient to do. We will load some libraries that will boost your data handling capacity.\n",
    "\n",
    "The main ones include `numpy` and `pandas`, which are the most prominent libraries to work efficiently with data in python. Here we just use the `import` function to, you guessed it import the pandas library and make it accessible `as` `pd` in the following code to save some typing (4 characters to be precise...). Then we load `matplotlib` and `seaborn` which are libraries that will help you to visualise the data. Visualisation of a dataset is key to getting a good understanding of what it is made before applying more involved machine learning algorithms. You will learn how handy it is to start formulating hypotheses and to evaluate output from data processing you will be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "\n",
    "# Module 2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Module 3\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Additional plotting functionality \n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D \n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "%matplotlib inline\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "print(\"libraries all imported, ready to go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The dataset is from a online retailer selling gifts and is based on a dataset taken from [here](https://archive.ics.uci.edu/ml/datasets/Online+Retail#).\n",
    "\n",
    "![Giftshop](img/giftshop.jpg)\n",
    "\n",
    "It is a transaction history of an online shop and as we will load it into python we will see that it comes with a set of feature descriptions:\n",
    "\n",
    "* `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "* `StockCode`: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "* `Description`: Product (item) name. Nominal.\n",
    "* `Quantity`: The quantities of each product (item) per transaction. Numeric.\n",
    "* `InvoiceDate`: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "* `UnitPrice`: Unit price. Numeric, Product price per unit in sterling.\n",
    "* `CustomerID`: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "* `Country`: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Loading the dataset\n",
    "\n",
    "In a first step we load the dataset with `pandas`. To achieve this you will use the `.read_csv()` method. We just need to point to the location of the dataset and indicate under what name we want to store the data, i.e. `retail`, and `pandas` will do the rest. In the `read_csv()` function, the `parse_dates` parameter is a boolean or list of ints or names or list of lists or dict, which by default is set to False. In this case, we are passing the name 'InvoiceDate' that represents the corresponding column. More details on how to use `parse_dates` can be found http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html. \n",
    "\n",
    "At a first stage, the data has only been loaded. Let's have a look at the top few lines - we can use the `.head()` method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data and explore the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We already learned one thing, customer `17850` bought 6 `WHITE METAL LANTERN`s on the 1st of December in the early morning. Perhaps a Christmas present?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also good practice to **always** check the dimensionality of the input data using the `shape` command to confirm that you really have imported all the data in the correct way (e.g. one common mistake is to get the separator wrong and end up with only one column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dimensionality of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "At this stage we do not really know what is going on in this dataset. We need to get beyond the first impression. How about trying to answer some simple questions like:\n",
    "\n",
    "* How many customers are we dealing with?\n",
    "* How many different products are being sold?\n",
    "* What country spends how much?\n",
    "* What has been the company's profit during the last year?\n",
    "* What period does the data span?\n",
    "\n",
    "We will go through these questions and learn new tricks as we move along. But before we get started, it is worth to have a little bit more background on what `pandas` is made of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection with Pandas\n",
    "\n",
    "Throughout this bootcamp you will be using `pandas` which is a python library that makes it muuuuuch more convenient to work with data than the base `python` methods. `pandas` is built on top of `numpy`, which is the library that brings efficient numerical operations to `python`. As the author of `pandas` Wes McKinney puts it: '*pandas provides high level data manipulation tools built on top of NumPy*'. `pandas` takes care of making it easy to work with tabular data in providing selections, merging, calculating statistics, filling in missing values and provides solutions to many other challenges that would be cumbersome to overcome with base `python`.\n",
    "\n",
    "When you load data into python with `pandas` it is put into a special structure called a `DataFrame`. `DataFrame`s are what makes `pandas` so convenient to work with for data analysis. It is worth to take the time to understand what kind of an object a `DataFrame` is, or in other words what it is made of in order to get all the benefits it has on offer.\n",
    "\n",
    "![Table Anatomy Class](./img/online_retail_table_anatomy.png)\n",
    "\n",
    "You are familiar with what a table is, it has column and rows and often these are annotated with column labels and row labels respectively. But how are table encoded with `pandas`. The raw data is stored in `numpy` **arrays** and this is where `pandas` can leverage all the numerical data processing.\n",
    "\n",
    "To add more convenient selection on top of this array, it is encoded in a so-called `pd.Series`, which can be thought of as a table with a single column. Crucially a `pd.Series` can have a (column) label and row labels. Also a `pd.Series` will store data of a given type, i.e. numbers, words, times. Row labels are called indexes in `pandas` and they are very important for a lot of `pandas` and we will introduce some later in the module. The `pd.Series` comes with many of the convenience functions that are included in `numpy`, such as `.sum()`, `.max()` etc. However, is has some additional functionality that `numpy` is missing. For instance it is very easy to count the unique number of entires in a pd.Series by simply using the `.nunique()` method.\n",
    "\n",
    "Finally a bunch of `pd.Series` in one table constitute the `DataFrame`, with column labels and an index. Crucially, `DataFrame`s can have different types of data in different columns, which is essential when representing tables. Within a `DataFrame`, the different columns of the table can easily be accessed via the *name* of the columns. Similarly, you can select individual rows via the indices.\n",
    "\n",
    "In this primer you will go though a lot of the basic pandas functionality and try to understand how they are build so it will be easier to maniputate them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - How many?\n",
    "\n",
    "Remember one of the questions from before? How many customers are we dealing with? Let's have a look.\n",
    "\n",
    "First we need to select the `CustomerID` column, which we do with the square brackets (`[]`). This yields a `pd.Series` only containing `CustomerID` column. From this we can then count the unique values with the `.unique()` function that is conventiently provided for `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the nunique() function on the column 'CustomerID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Activity \n",
    "\n",
    "Let's repeat this to find out how many countries the customers are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the nunique() function on the column 'Country'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find out how many different products the shop is keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the nunique() function on the column 'StockCode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity\n",
    "\n",
    "Perhaps you have noticed that repeating the same operation again and again can become annoying. Let's be more efficient here.\n",
    "\n",
    "The remedy is `pandas`' `.apply()` method, which allows you to apply a function on all the columns with a one line command. To do this we need to just select `retail` `Dataframe` and specify what function we want to apply. Before we were always using the `.nunique()` method, so let's do the same. In this context you need to be a bit more precise on where that function is form. As mentioned before, the `.nunique()` method works on pd.Series and that is how you will find it under `pd.Series.nunique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the count of unique values for all the columns using the apply() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "The `apply()` function will automatically go through all the columns and return the number of unique values in each column. We will be relying on the `.apply()` method later on when we calculate features from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A customer's basket\n",
    "\n",
    "You should have a look at a transaction in more detail to see how it is structured. Let's say we were interesting in having a closer look at the invoice with `InvoiceNo` `544182`. How would you filter just the rows of the `retail` table that has records on that specific invoice?\n",
    "\n",
    "To do this we need to learn how to filter the dataset with `pandas`. The syntax for filtering is a bit cumbersome but it makes a lot of sense once you get the hang of it. To get it you need to understand that you can select from a `pd.DataFrame` in providing a `pd.Series` of `True` and `False` values. When doing so, `pandas` will only return the rows of the `pd.DataFrame` where `pd.Series` was True and will discard the rest.\n",
    "\n",
    "Thus filtering a table is a two step process: \n",
    "\n",
    "* you build a `pd.Series` that indicates the rows that fit your condition\n",
    "* then you select these rows from the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out and display the rows for the invoice number 544182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "Filtering can also be used to select for rows that are in a give numberic range. For instance try to filter the `invoice` `DataFrame` for just those items with a `UnitPrice` above £5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out all invoices where the 'UnitPrice' is above 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Activity\n",
    "\n",
    "How much did the customer spend in total? From our dataset two columns will be very helpful to calculate the total send, i.e. `Quantity`, the number of items that were bought and `UnitPrice`, the amount each item cost. If we multiply these columns with each other we get how much the customer spent per item type and we only need to sum up the cost for all items.\n",
    "\n",
    "Let's start with the total price per item. With `pandas` this is straight forward as we can simply select columns and multiply them with each other. Make sure to store the resulting `pd.Series` in a variable. Then we calculate the sum of all the item prices using the `pandas` method `.sum()`; remember that we can do this because the data is stored in a `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start by multiplying the invoices' 'Quantity' by the 'UnitPrice' and then sum everything up with .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, this customer spent a total of £850 in this invoice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "What is the total turnover of the retail store. In essance it is the same calculation, but be careful to use the right `pd.Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the total turnover of the retail store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A customer's history\n",
    "\n",
    "Knowing the total spend for one invoice is good, but this will not teach us much about this customer in general. Let's also look at all the transactions of that customer (`CustomerID` `18257`).\n",
    "\n",
    "You will first have to select all the rows in the table that are associated with the customer vis the `CustomerID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out and display the rows for the customer 18257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many purchases did this customer do? You should be able to do this with what we learned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": true
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of unique invoices for the customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the total spent for purchase (invoice) '563569' of this customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out the invoices with InvoiceID equal to '563569'\n",
    "# Find the product cost \n",
    "# Calculate the total cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that repeating these operations can become quite cumbersome. We should write a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 2 - Writing simple functions\n",
    "\n",
    "`python` contains a whole range of predefined functions that makes is so useful, and we have been using some. Missing functionality is quickly added when loading libraries and so far we have mostly relied on such functions. Here we will learn how to write our own simple functions that help us to simplify our tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some simple pseudo code of a function. Generally speaking a function will take some input, for example some `data` on which it performs an operation. In this case the simple `.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'def' defines the function where simple_sum is its name and data is a parameter\n",
    "\n",
    "def simple_sum(data):\n",
    "    output = data.sum()  # here the body of the function starts and its operation is performed\n",
    "    return output  # finally the function needs to return the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "At this stage we have defined the function. In order to use it, we need to call it in a way that should be very familiar to you already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call the 'simple_sum' function specifying the 'data' parameter\n",
    "\n",
    "simple_sum(data = invoice['UnitPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "source": [
    "This function is very simple and pointless really as we could simply have done `invoice['UnitPrice'].sum()`. However you can change the crucial part of the function to anything you like, i.e. the portion where we perform an operation on the data.\n",
    "\n",
    "In this way functions offer you to extend the functionality of python. Remeber until now you have been using many predefined python functions that made your life easier. The functions that you will write in the follwing exercises are for exactly the same purpose, to make your life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A customer's history at high throughput\n",
    "\n",
    "What if we want to know the total spent by the customer for each purchase in a more systematic way? So far we had to repeat this operation for every purchase of the customer. This will be a two step process:\n",
    "\n",
    "* first we group the data on a per-invoice-basis\n",
    "* then we apply a function that calculates the total spend per group\n",
    "\n",
    "This is a very commonly used framework to calculate summary statistics from data. In the following part of the notebook, we will apply this strategy to calculate the summary statistics for the total spent per invoice, and per country and per time period.\n",
    "\n",
    "For the grouping, we can use a very handy function of `pandas` allows to go though the data on a per-invoices basis. Enter the `.groupby()` method.\n",
    "\n",
    "![Group by operation](img/online_retail_groupby.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby InvoiceNo\n",
    "The group `.groupby()` call is very simple, we just need to specify the column that we would want to group by. In this case it is the `InvoiceNo` (Note: you can also group by multiple columns and we will see that a bit later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group the customers history by its 'InvoiceNo's and display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the background the `InvoiceNo` column was converted into a sorted index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the function\n",
    "\n",
    "After grouping we can then apply an operation to each to and `.apply()`. Let's first write a simple function that calcualtes the total spent, inspired with what we have already done in a previous learning activity.\n",
    "\n",
    "We first need to define the functions with...\n",
    "\n",
    "    def total_spend(data):\n",
    "        ---> operation goes here <---\n",
    "        return pd.Series({'column_name': column_values)\n",
    "    \n",
    "...where `data` is the data input and we `return` a `pd.Series` where we have set a `column_name` (Note: You could also simply return the the `column_values` without passing them in a `pd.Series` object, but then you would lose the option to give a `column_name`).\n",
    "\n",
    "Your task now is just to build a function that fits in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function called 'total_spent' that takes 'data' as a parameter\n",
    "# where the operation calculates the amount spent per product. Remember\n",
    "# that you can do 'Quantity' time 'UnitPrice'). Finally '.sum()' the output\n",
    "# and return it as a pd.Series.\n",
    "\n",
    "def total_spent(data):\n",
    "    product_spend = data['Quantity'] * data['UnitPrice']\n",
    "    total_spent   = product_spend.sum()\n",
    "    return pd.Series({'total_spent': total_spent})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have defined the function and it hosts exactly the same operation as in a previous learning function, with the only difference that it is contained withing a neat function.\n",
    "\n",
    "We can now `.apply()` it to our grouped `pd.DataFrame`.\n",
    "\n",
    "![Apply the total_spent function](img/online_retail_apply_total_spent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the total_spent() function on the grouped customers\n",
    "# Reset the index\n",
    "# Display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can confirm from above where you calculated the two invoices (namely `544182` and `563569`), we get the same results. Crucially, we now have all the customer's total spends and we even know how many times the customer instigated a purchase. The result is presented in a neat pd.Series.\n",
    "\n",
    "We can still do better in resetting the index that was set during the `.groupby()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus activity\n",
    "\n",
    "Can you sort the output of the `pd.DataFrame`, to make the table easier to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the values of total_spent in an increasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Tables to plots\n",
    "\n",
    "Tables are a visual way (albeit primitive) to look into a dataset. There is much more that we can do to simplify getting insights from the data and to communicated it clearly to others.\n",
    "\n",
    "We will be using the `seaborn` library for most plots in this bootcamp. It is based on the `python` classic `matplotlib`, which we loaded earlier. In a nutshell we define plot objects that can have a variety of properties, e.g. type of plots, dataset, data mapped to x-axis etc. Then we use `seaborn`'s helper functions that take care of most of the plotting setup to produce a graph where many reasonable presets have been set.\n",
    "\n",
    "In this way there is a whole range of plot types that can be quickly produced. Have a look [here](https://stanford.edu/~mwaskom/software/seaborn/examples/index.html) for a gallery of plots possible with `seaborn`.\n",
    "\n",
    "![Seaborn gallery](img/seaborn_gallery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce our first plot.\n",
    "\n",
    "To do this we first initiate a figure object. Here we can set some parameters such as figure dimentsions, but there are many more to pick from [here](http://matplotlib.org/api/figure_api.html). At this stage we have an emply plot.\n",
    "\n",
    "When we build a plot we need to ask yourself three things:\n",
    "\n",
    "* What data do I want to plot?\n",
    "* What type of plot is suited for the data?\n",
    "* What aestetic (x- or y-axis, colour etc.) to I pick to reprent a given dimension of the data on the plot type?\n",
    "\n",
    "The dataset is straightforward, it is the `total_spent` per invoice of a given customer. The a barplot is quite suitable and we call it with `sns.barplot()`. It comes with a few vary important parameters. The `data=` parameter is where we define the dataset that we want to plot and the `x=` and `y=` are the aestetics (or visual aides) of the plot where we **map** a choice of dimensions. Namely we want to have one bar per invoice, hence we split the `InvoiceNo` over the `x`-axis and we want the height of the bars to represent the `total_spent`.\n",
    "\n",
    "Finally we print the plot with `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the barplot of the total spent per invoice number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus activity\n",
    "\n",
    "Try to order the bars when plotting them. You could again use the `.sort_values()` mehtod.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the barplot of the total spent per invoice number using an increasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity - Country spending\n",
    "\n",
    "Let's what country is spending the most. Here again `.groupby()` will come in handy, as well as the function `total_spent()` that defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform group by country\n",
    "# Apply total spent in each grouped category\n",
    "# Resetting index\n",
    "# Sorting by total spent\n",
    "# Present the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that could easily reuse our function to produce a completely new result based on the same operation. The only thing we changed was the group - we grouped by `Country` instread of `InvoiceNo`.\n",
    "\n",
    "Let's further clean up the result in converting the `Country` index into a column for more convenient plotting and let's sort the values to a clearer message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again use a barplot to visualise this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the total spent per country using a barplot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above show that clearly most of the money is pent in the UK. Perhaps it would be a good idea to use a **log-scale** here to distinguish between the lower ranking countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat the plot by this time apply a log scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - A history of  purchases\n",
    "\n",
    "Up until now, you have learned a lot about the data, but we do not yet know much about what time period the dataset is spanning? To get an impression we have the `IncoiceDate` column to play with. At this point we can conveniently calculate the total_spend per time unit. Let's start with the time unit in which the data was loaded, simply on 'InvoiceDate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform group by 'InvoiceDate'\n",
    "# Apply total spent in each grouped category\n",
    "# Display the some of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the apply was performed on a minute basis, which might not be the most useful (and takes a bit of time). \n",
    "\n",
    "Let's calcualte the `total_spent` on a monthly basis. Notice that this time we can not reset the index (with `.reset_index()`) as we need the indexed form to apply the `.resample()` method, that allows to switch between time resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the '.resample()' method from pandas with the 'M' parameter to collect\n",
    "# the monthly total spend and '.sum()' it all up in one line\n",
    "\n",
    "# Reset the index\n",
    "\n",
    "# Present the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this data. This time we will not use `seaborn` but a ususal suspect, that beyond making data maniplations more convenient (hint, hint), has been developed to work very well with timeseries data. You guess right, `pandas` comes with its own plotting functionality and it is ver easy to plot time series.\n",
    "\n",
    "We simply use the `.plot()` method on the `pd.DataFrame` this time specifying that we want to use the index as our x-axis (remember, `seaborn` does not play well with `pandas` indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the total spent per month "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset spand a period from December 2010 to August 2011. We can also see that there are fluctuations over the year and also within the weeks probably suggesting particular spending patterns for weekend days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity\n",
    "\n",
    "Repeat the opeartion to get the `total_spent` on a dayly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the total spent per day \n",
    "# '.resample()' with the 'D' parameter and .sum() up the results before plotting the time series with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now equipped with all the tools to build a dataset that summarised all the imporant information the intial dataset that you were given. It will help to systematically understand customer behaviour. In the next modules we will use the dataset that you generated to do customer segmentation and predictions to try and classify whether customers are likely to be returning to the online shop.\n",
    "\n",
    "![Visual Representation of Dataset](img/online_retail_visual_table.png)\n",
    "\n",
    "There will be a two step building process to the dataset:\n",
    "\n",
    "* aggregation based on `InvoiceNo`\n",
    "* aggregation based on `CustomerID`\n",
    "\n",
    "![2-step Aggregation](img/online_retail_two_step_aggregation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Summarise Invoices\n",
    "\n",
    "You will start by summarising all the information contained in the invoices. To extract as much meaningful information as possible, you define custom functions that summarise different aspects of customer behaviour. This is a crucial step in any data science endvour and is called **feature engineering**.\n",
    "\n",
    "Let's start with a function that we already know: `total_spent`. We will essentially re-use the same function as before, but we need to give it a little update. This time we will want to make sure that when we look at items purchases that they have a positive `Quantity` associated with them. We can achieve this by updating the functions operation to filter for rows in the `pd.DataFrame` where `Quantity > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a slight modification to the total_spent function you built above\n",
    "\n",
    "def total_spent(data):\n",
    "    purchased = data[(data['Quantity'] > 0)]  # make sure to filter for rows where an item was sold (not returned)\n",
    "    spent = purchased['Quantity'] * purchased['UnitPrice']\n",
    "    return spent.sum()  # we sum up the sending per items and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further little tweak is concerned with the way that the function returns the computed feature. If you remember in the previous version of the function, we returned the result in a `pd.Series` object. This was so that we could assign a column name to the output. This time we want to just return the simple calculated values as we will let the naming be done by an additional convenience function, which single role is to build a neat final `pd.DataFrame` (see below).\n",
    "\n",
    "But before we get into that, let's define some more features and functions that can caluclate them. It could be interesting to see how much customers have been refunded. This function will be very simular to the the `total_spent` and we can name it `total_refunded`. The single difference is that we are this time interested in the entries of the table where there was a negative `Quantity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function is very similar to the one above, except that it records\n",
    "# the number of items that were returned (rather than sold)\n",
    "\n",
    "def total_refunded(data):\n",
    "    returned = data[(data['Quantity'] < 0)]\n",
    "    refunded = returned['Quantity'] * returned['UnitPrice']\n",
    "    return refunded.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature could be the total number of items purchases. This function will be called `total_items()` and operates in a similar way, except that it ignores `UnitPrice`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TEST ACTIVITY \n",
    "\n",
    "# Define a function called 'total_items' that takes data as a parameter.\n",
    "# It should filter all rows where the 'Quantity' is more than 0, i.e.\n",
    "# where an item was sold (not returned). Then it should select the \n",
    "# 'Quantities' column and return the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the corresponding function that is concerned with refunded items can be quickly coded up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function is very similar to the one you just write, except that it\n",
    "# records the number of items that were returned (rather than sold)\n",
    "\n",
    "def total_items_returned(data):\n",
    "    n_items = data[(data['Quantity'] < 0)]['Quantity']\n",
    "    return n_items.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, all the functions defined will just return a `pd.Series` that does not have a name associated with it. That is there the `compute_invoice_metrics` functions comes in. This function is required because with the `.apply()` method in `pandas` we can only call a single function. `compute_invoice_metrics` is a trick to circumvent this limitation in that it is a single function that calls other functions as it is called. Furthermore it naming the output from the other functions and return the result per group in a `pd.Series` that is stiched up nicely to become the final `pd.DataFrame` that is output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This functions defines what other functions (which we defined above) we want to \n",
    "# apply to the grouped dataset\n",
    "\n",
    "def compute_invoice_metrics(data):\n",
    "    result = {\n",
    "        'total_spent': total_spent(data),\n",
    "        'total_refunded': total_refunded(data),\n",
    "        'total_items': total_items(data),\n",
    "        'total_items_returned': total_items_returned(data)\n",
    "    }\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the required functions defined, we can start with the processing. First we will group the data with a `.groupby()`. This time we will use multiple columns by which we want to group. (Note: Although we know that each invoice has a unique `InvoiceNo`, we also add other columns at this stage. This only way to keep these columns as additional output in the resulting table beyond the features that our custom functions calculate. We want to keep these columns are we need to use them in the next aggregation step when we produce more features on a **per-customer-basis**.) \n",
    "\n",
    "Then we apply the `compute_metrics()` function to the grouped table. (**Note: This can take some time!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group the invoices this time by multiple columns including\n",
    "# all of 'InvoiceNo', 'InvoiceDate', 'CustomerID' and 'Country'.\n",
    "\n",
    "# Apply the 'compute_invoice_metrics' you defined above\n",
    "\n",
    "# Reset index\n",
    "\n",
    "# Display the first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Activity\n",
    "\n",
    "Can you design additional features that you think could be useful? Give it a go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Summarize customers\n",
    "\n",
    "You will now have a table where every row has a single `InvoiceNo` with various summaries or features associated with that number. However we are interested in better understanding our customers. So we need to apply a further summarising step to get to each row being a single `CustomerID`. Then we can learn behavioural aspects or customers.\n",
    "\n",
    "This time you will calculate a time-based feature. To be specific you will learn every how may days a customer places an order. This function is a bit more involved as it has to deal with considerations that are specific to time data. The details do not matter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function records the time that has passed between two orders\n",
    "\n",
    "def time_between_orders(data):\n",
    "    t = data[(data['total_items'] > 0)]['InvoiceDate']  # filter for the rows where an item was sold (not returned)\n",
    "    t = data['InvoiceDate'].sort_values()  # make sure to sort values according to date\n",
    "    timedelta = t - t.shift()  # shift by one row\n",
    "    days = timedelta.astype('timedelta64[D]')  # convert to n. days\n",
    "    return days.mean()  # return the average time between orders (to aggregate in case there were many orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have a summarising function `compute_customer_metrics()` that will be called with the `.apply()` to in turn call all the functions that actually calculate the features. This time we mostly `.sum()` up the individual invoices' totals to get to a value that reflect the customer as a whole. Features that we calculate on a per-customer-basis should include:\n",
    "\n",
    "* total_spent\n",
    "* total_refunded\n",
    "* total_items\n",
    "* total_items_returned\n",
    "* min_spent\n",
    "* mean_spent\n",
    "* max_spent\n",
    "* balance\n",
    "* n_orders\n",
    "* time_between_orders\n",
    "\n",
    "Here you are provided with a skeleton of `compute_customer_metrics()` but it is incomplete. Add the `time_between_orders()` and also add the final `balance` per customer where it is defined as the `total_spent` - `total_refunded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This functions again defines what other functions we want to apply to the\n",
    "# grouped dataset, this time on a per customer basis\n",
    "\n",
    "def compute_customer_metrics(data):\n",
    "    result = {\n",
    "        'total_spent': data['total_spent'].sum(),\n",
    "        'total_refunded': data['total_refunded'].sum(),\n",
    "        'total_items': data['total_items'].sum(),\n",
    "        'min_spent': data['total_spent'].min(),\n",
    "        'mean_spent': data['total_spent'].mean(),\n",
    "        'max_spent': data['total_spent'].max(),\n",
    "        'balance': data['total_spent'].sum() + data['total_refunded'].sum(),  # TODO: REMOVE (refunds are -ve values)\n",
    "        'n_orders': len(data),\n",
    "        'time_between_orders': time_between_orders(data)  # TODO: REMOVE\n",
    "    }\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again you will group the dataset that you want to aggregate, which this time is the invoices `pd.DataFrame`. And finally you apply the `compute_customer_metrics()` function you defined and reset the indices. (Note: This can again take some time, but should be faster than the aggregation before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group by Customer ID and Country\n",
    "\n",
    "# Apply the compute_customer_metrics() function in the grouped instances\n",
    "\n",
    "# Display the first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last you have gone from an item-based table to an information rich customer-based table that you can now supply to unsupervised and supervised machine learning algorithms to try and learn something about the customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just *one* more thing... *Quality control*. We can immediately see that for instance for the `time_between_orders` column we have some missing data as the `count` value is lower than that of the other columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Imputation\n",
    "\n",
    "We should first take care to replace the missing values in the dataset as they prevent machine learning algorithms to run. There are many strategies to help with missing data and they depend on whether the missing data is numeric or categorical.\n",
    "\n",
    "* simply removing rows where there is missing data (e.g. `.dropna()` can achieve this)\n",
    "* imputing the values with a summary statistic such as mean or median or most frequent value (e.g. `Imputer` from `sklearn` module)\n",
    "* replace the values with a resonable estimate\n",
    "\n",
    "What strategy is best for you problem very much depends on the specifics of your dataset. However generally speaking it is not worth to remove large chunks of data.\n",
    "\n",
    "In our case the missing values are exclusively found in the `time_between_orders` column, so we should have a look at these rows where this occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the instances where the time_between_orders is empty\n",
    "\n",
    "# How many nan cases do we have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing the table we can see that only in the rows where we have a single order (`n_orders == 1`) that the `time_between_orders` is `NaN`, i.e. Not a Number. That makes immediate sense and indicates that these customers have not yet returned for anther purchase.\n",
    "\n",
    "A reasonable strategy here would be to replace the `NaN` values by the longest time period (in days) that we would expect a customer to be returning, e.g. 365 days. So let's replace all the `NaN` values with the `pandas` method `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the empty values (NAs) from the column 'time_between_orders' and fill them with the value 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Removing Outliers\n",
    "\n",
    "Now let's move on to taking care of the blatant outliers. An outlier is an observation that appears extreme relative to the rest of the data. Some ML techniques are sensitive to outliers and it's better to remove these samples before proceeding.\n",
    "\n",
    "There are again many strategies to deal with this scenario. The real question that we need to answer is at what point we consider a value extreme and whether it is really legitimate to remove it from the observations.\n",
    "\n",
    "Here we have defined a simple function that provides for a straighforward way of removing observations that are `k` standard deviations (`sigma`) away from the mean (`mu`) of a distribution.\n",
    "\n",
    "Assuming that the data is normally distributed, approximately 99.7% (almost everything) of the data falls within three standard deviations of the mean. \u001bUnder this assumption we are considering as outliers only samples with very unlikely values for a features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function defines what datapoints we consider \n",
    "\n",
    "def remove_outliers(data, k=3):\n",
    "    mu = data.mean()  # get the mean\n",
    "    sigma = data.std()  # get the standard deviation\n",
    "    filtered_data = data[np.abs((data - mu) / sigma) < k]  # filter values based on distance from mean\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `.apply()` this function. In case that the value is decalred an outlier, its value is replace by `NaN`, keeping the structure of the `pd.DataFrame` intact. However it only operates on numerical columns. Therefore, we first need to some `pd.DataFrame` processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the remove_outliers function to the customers dataframe and display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove the `NaN` values. This can easily be achieve with the `.dropna()` method that takes care of all the rows with a single occurence of `NaN` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Scaling\n",
    "\n",
    "Often when we are working with multidimensional data, the data have different units and thus exist on different scales. When comparing the data internally, or when the values map to similar space, then that is not a problem. However if a dimensions is in the millions, e.g. population of a country, and an others dimension in the few hundreds, e.g. number of hospitals, then there can be an uneven impact of the dimensions with higher values.\n",
    "\n",
    "You can easily visualise this with a `boxplot`. `boxplot` represent essential statistics that describe distributions; from bottom to top, the horizontal lines of the box represent the first quartile (`Q1`), the median and third quartile (`Q3`). The distance between `Q1` and `Q3` is called inter quartile range (`IQR`). The whiskers of the boxes on the top and bottom are defined as `Q1 - 1.5 x IQR` and `Q3 - 1.5 x IQR` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot a sns.boxplot() of the customer dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance you can see that `n_orders` is defined in a much narrower space than `balance`. If you were to use the data in an unscaled form, the effect of `balance` might be disproportionnaly high.\n",
    "\n",
    "To account for this you can scale your data, so that all the dimensions fall onto the same space. We use a simple function from the `sklearn` library for this purpose. Namely we use the `StandardScaler()`. In the coming sections of the bootcamp we will be using `sklearn` extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise the scaler\n",
    "\n",
    "# Apply auto-scaling (or any other type of scaling) and cast to DataFrame \n",
    "\n",
    "# Print the first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replot the `boxplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replot the boxplot with the scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Exporting a csv\n",
    "\n",
    "Now that we have produced a dataset that is ready for applying some machine learning algorithms we will save the it to disk. This also served as a checkpoint for the bootcamp so that you can get started straight away with the next module even if you got stuck with some part above.\n",
    "\n",
    "Writing a `pd.Dataframe` to disk is very easy - you just use the `.to_csv()` method, and specify the file path to where you want it saved. There also other [formats](http://pandas.pydata.org/pandas-docs/stable/api.html#id12) that you save to, which are based on functions that work in exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Save to a csv file with the '.to_csv()' method and give the file a name you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3\n",
    "\n",
    "## Clustering with K-Means\n",
    "\n",
    "K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. Given an initial set of K centers, the K-means algorithm alternates the two steps:\n",
    "\n",
    "1. for each center we identify the subset of training points (its cluster) that is closer to it than any other center;\n",
    "2. the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster.\n",
    "\n",
    "These two steps are iterated until the centers no longer move or the assignments no longer change. Then, a new point x can be assigned to the cluster of the closest prototype.\n",
    "\n",
    "### Learning Activity - Run K-Means with two features\n",
    "Isolate the features `mean_spent` and `max_spent`, then run the K-Means algorithm on the resulting dataset using K=2 and visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Appy k-means with 2 clusters using a subset of features (mean_spent and max_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function generates a pairplot enhanced with the result of k-means\n",
    "\n",
    "def pairplot_cluster(df, cols, cluster_assignment):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        df, dataframe that contains the data to plot\n",
    "        cols, columns to consider for the plot\n",
    "        cluster_assignments, cluster asignment returned by the clustering algorithm\n",
    "    \"\"\"\n",
    "    # seaborn will color the samples according to the column cluster\n",
    "    customers['cluster'] = cluster_assignment \n",
    "    sns.pairplot(df, vars=cols, hue='cluster')\n",
    "    customers.drop('cluster', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation between the two clusters is neat (the two clusters can be separated with a line). One cluster contains customers with a low spendings and the second with high spendings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Run K-Means with all the features\n",
    "Run K-Means using all the features available and visualise the result in the subspace `mean_spent` and `max_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Appy k-means with 2 clusters using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is now different. The first cluster contains customers with a maximum spending close to the minimum mean spending and the second contains customers with a maximum spending far from the minimum mean spending. This way can tell apart customers that could be willing to buy object that cost more than their average spending.\n",
    "\n",
    "***Question***: Why can't the clusters be separated with a line as before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning activity - Compare expenditure between clusters\n",
    "\n",
    "Select the features `'mean_spent'` and `'max_spent'` and compare the two clusters obtained above using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare the expenditure between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Compare mean expediture with box plot\n",
    "\n",
    "Compare the distribution of the feature `mean_spent` in the two clusters using a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare mean expediture with box plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Looking at the centroids\n",
    "\n",
    "Look at the centroids of the clusters `kmeans.cluster_centers_` and check the values of the centers in for the features `'mean_spent', 'max_spent'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare the centroids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note:\n",
    "- Cluster 0 contains more customers.\n",
    "- Customers in cluster 1 spend more in average, but have a more changeble behaviour.\n",
    "- Customers in cluster 1 place more order and ask for more refunds.\n",
    "\n",
    "We can study the averages also looking at the centroids:\n",
    "\n",
    "***K-Means, pro and cons***\n",
    "\n",
    "Pro\n",
    "- fast, if your dataset is big K-Means might be the only option\n",
    "- easy to understand\n",
    "- any unseen point can be assigned to the claster with the closest mean to the point\n",
    "- many implementsions available\n",
    "\n",
    "Cons\n",
    "- you need to guess the number of clusters\n",
    "- custers can be only globular\n",
    "- the results depends on the initial choice of the means\n",
    "- all the points are assigned to a cluster, clusters are affected by noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Compute the silhouette score\n",
    "Compute the silhouette score of the clusters resuting from the application of K-Means.\n",
    "\n",
    "The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``. It represents how similar a sample is to the samples in its own cluster compared to samples in other clusters.\n",
    "\n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the silhouette score of k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score is pretty high, we can say that the clusters are compact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering: Linking with Linkage\n",
    "\n",
    "The main idea behind hierarchical clustering is that you start with each point in it's own cluster and then\n",
    "\n",
    "1. compute distances between all clusters\n",
    "2. merge the closet clusters\n",
    "\n",
    "Do this repeatedly until you have only one cluster.\n",
    "\n",
    "This algorithm groups the samples in a bottom-up fashion and falls under the category of the agglomerative clustering algorithms.\n",
    "\n",
    "According to the distance between clusters and samples that one choose the clusters will have different properties. In this section we'll use a distance that will minimizes the variance of the clusters being merged.\n",
    "\n",
    "This algorithm results in a hierarchy, or binary tree, of clusters branching down to the last layer which has a leaf for each point in the dataset that can be visualise with a \"Dendrogram\". The advantage of this approach is that clusters can grow according to the shape of the data rather than being globular.\n",
    "\n",
    "sklearn implements hierarchical clustering in the class `sklearn.cluster.AgglomerativeClustering` (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering), this class is mainly a wrapper to the functions in `scipy.cluster.hierarchy` (http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Plotting dendograms\n",
    "Use the function `linkage()` from `scipy.cluster.hierarchy` to cluster the retail data and pass the result to the function `dendrogram()` to visualise the result. Trunc the dendrogram if the initial result is unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply hierarchical clustering \n",
    "\n",
    "# Draw the dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coloring of the figure highlights that the data can be segmented in two big clusters that were merged only in the very last iterations of the algorithm. But, if we look close, we can spot another smaller cluster that was merged to the red one at a distance of around 40.\n",
    "\n",
    "We can improve the readability of the dendrogram showing only the last merged clusters and a threshold to color the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw the dendrogram using a cut_off value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we able to see the threshold that reflects the color to the clusters and we easily realise that the closer it is to zero, the more clusters are highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Running Linkage with Sklearn\n",
    "\n",
    "Use `sklearn.cluster.AgglomerativeClustering` to cluster the retail data according to the 3 clusters highlighted by the dendrogram above and visualise the result in the subspace give by the features `mean_spent` and `max_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform clustering with AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is similar to the one we had with K-Means, but now we also tell apart customers that moderately deviate from their average with their maximum spenging and customer that strongly deviate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity - Visualising the clusters in 3D\n",
    "Create a 3D chart where the results of the Linkage algorithm is shown in the space formed by the features `min_spent`, `max_spent` and `mean_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function generates a 3D plot enhanced with the result of clustering\n",
    "\n",
    "def scatter_cluster3d(x, y, z, cluster_assignment, fig):\n",
    "    ax = Axes3D(fig)\n",
    "    for cluster in np.unique(cluster_assignment):\n",
    "        ax.scatter(x[cluster_assignment==cluster], \n",
    "                   y[cluster_assignment==cluster], \n",
    "                   z[cluster_assignment==cluster],\n",
    "                   c=sns.color_palette()[cluster], label='cluster '+str(cluster))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters in 3D using the scatter_cluster3d() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this space we can see that cluster 1 has less variablity respect to cluster 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Activty - Interactive 3D visualisation with Plotly\n",
    "\n",
    "Recreate the 3D plot above with Plotly.\n",
    "\n",
    "Here are some example to inspire your code: https://plot.ly/python/3d-scatter-plots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an interactive 3D plot enhanced with the result of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can navigate this chart with you mouse and hide/show the cluster clicking on the legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hierarchical clustering, pro and cons***\n",
    "\n",
    "Pros\n",
    "- The clusters are not globulars anymore\n",
    "- Doesn't depend on initial random choices\n",
    "- The dendrogram shows a good summary of how the algorithm works\n",
    "\n",
    "Cons\n",
    "- Slower than K-Means\n",
    "- We still need to choose the number of clusters\n",
    "- Still, the clusters are affected by noisy points\n",
    "- Assigning a new point to a cluster is not straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster. \n",
    "\n",
    "Summary of the Algorithm:\n",
    "\n",
    "- starts with an arbitrary starting point and retrieved all the points in the radius of distance `eps` from it \n",
    "    - if the radius contains `min_samples` points, start a cluster\n",
    "      - add all the points in the radius of distance `eps` to the cluster and their `eps` neighbors.\n",
    "      - continue expanding the cluster iterating on the the procedure on all the neighbors\n",
    "    - otherwise mark it as noise/outlier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sklearn implementation doc: http://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
    "\n",
    "Animated DBSCAN: http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Activity - A starting value for eps\n",
    "\n",
    "Measure the distance of each point to its closest neighbor using the function `sklearn.metrics.pairwise.pairwise_distances` (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) and plot the distribution of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distance of each point to its closest neighbor\n",
    "\n",
    "# Plot the distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the distance will help us choose a starting point for `eps`. We see that it's very likely that a point as at least one neighbour in a radius of 0.15 and that only very few point have it at distance 2.5. Since we want that a core point has more than one point in is `eps`-neighborhood we can start picking `eps` on the right tail of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Activity - Applying DBSCAN\n",
    "\n",
    "Cluster the customer data with DBSCAN and visualise the results in the subspaces used for the other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters using pairplot_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN clustered all the points in one big cluster and marked as outiers all the points that are not in dense areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the clusters in 3D using the scatter_cluster3d() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that DBSCAN grouped most of the samples in 1 big cluster and maked samples at the border of this space as outliers. Which means that DBSCAN acted as an outlier detection algorithm more than a clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning activity - Compute the silhouette score of the DBSCAN cluster\n",
    "\n",
    "Compute the silhouette score of the clusters made with DBSCAN and compare it with the silhouette score achieved with K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the silhouette score of DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity -  How many clusters with DBSCAN?\n",
    "\n",
    "Vary `eps` and `min_samples` and study how the number of clusters varies as result. This way we'll have an idea of how many cluster we get varying the parameters. This can help us choose the parameters if we already have an idea of how many clusters we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many clusters with DBSCAN?\n",
    "# WARNING this may take a couple of minutes to finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DBSCAN, pro and cons***\n",
    "\n",
    "Pros\n",
    "- The clusters are not globulars anymore\n",
    "- We don't have to chose the number of clusters\n",
    "- Fast, few clustering algorithms can tackle datasets as large as DBSCAN can.\n",
    "- Has an embedded concept of noise (outliers)\n",
    "\n",
    "Cons\n",
    "- `eps` and `min_samples` can be hard to tune\n",
    "- less intuitive than K-Means or Linkage\n",
    "- assigning an unseen sample to a cluster is not straightforward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
